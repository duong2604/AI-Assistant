import { NextRequest } from "next/server";
import OpenAI from "openai";

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(req: NextRequest) {
  try {
    const { supplier, invoiceExtract } = await req.json();

    const validateInvoiceSchema = {
      type: "object",
      additionalProperties: false,
      required: ["validation_results", "overall_status"],
      properties: {
        validation_results: {
          type: "array",
          description: "Danh s√°ch k·∫øt qu·∫£ ki·ªÉm tra t·ª´ng tr∆∞·ªùng.",
          items: {
            type: "object",
            additionalProperties: false, // üëà B·∫ÆT BU·ªòC c√≥ d√≤ng n√†y
            required: [
              "field",
              "supplier_value",
              "invoice_value",
              "status",
              "comment",
            ],
            properties: {
              field: {
                type: "string",
                description:
                  "T√™n tr∆∞·ªùng ƒë∆∞·ª£c ki·ªÉm tra, v√≠ d·ª•: supplier_name, tax_code, bank_account",
              },
              supplier_value: {
                type: ["string", "null"],
                description: "Gi√° tr·ªã t·ª´ danh m·ª•c nh√† cung c·∫•p",
              },
              invoice_value: {
                type: ["string", "null"],
                description: "Gi√° tr·ªã ƒë·ªçc ƒë∆∞·ª£c t·ª´ h√≥a ƒë∆°n",
              },
              status: {
                type: "string",
                enum: ["match", "mismatch", "missing", "unknown"],
                description: "Tr·∫°ng th√°i so kh·ªõp",
              },
              comment: {
                type: "string",
                description: "Nh·∫≠n x√©t ho·∫∑c gi·∫£i th√≠ch k·∫øt qu·∫£ ki·ªÉm tra",
              },
            },
          },
        },
        overall_status: {
          type: "string",
          enum: ["valid", "invalid", "partially_valid"],
          description: "T·ªïng k·∫øt k·∫øt qu·∫£ ƒë·ªëi chi·∫øu h√≥a ƒë∆°n",
        },
      },
    };

    const events = await openai.responses.create({
      model: "gpt-4o-mini",
      input: [
        {
          role: "system",
          content:
            "B·∫°n l√† h·ªá th·ªëng ki·ªÉm tra h√≥a ƒë∆°n. H√£y so s√°nh d·ªØ li·ªáu nh√† cung c·∫•p v√† d·ªØ li·ªáu tr√≠ch xu·∫•t t·ª´ h√≥a ƒë∆°n.",
        },
        {
          role: "user",
          content: JSON.stringify({
            supplier,
            invoiceExtract,
          }),
        },
      ],
      text: {
        format: {
          type: "json_schema",
          schema: validateInvoiceSchema,
          name: "supplier_invoice_validation",
        },
      },
      stream: true,
    });

    // Create a ReadableStream that emits SSE data
    const stream = new ReadableStream({
      async start(controller) {
        try {
          for await (const event of events) {
            // Sending all events to the client
            const data = JSON.stringify({
              event: event.type,
              data: event,
            });
            controller.enqueue(`data: ${data}\n\n`);

            if (event.type === "response.output_text.delta") {
              process.stdout.write(event.delta);
            } else if (event.type === "response.completed") {
              console.log("\n Done!");
            }
          }
          // End of stream
          controller.close();
        } catch (error) {
          console.error("Error in streaming loop:", error);
          controller.error(error);
        }
      },
    });

    // Return the ReadableStream as SSE
    return new Response(stream, {
      headers: {
        "Content-Type": "text/event-stream",
        "Cache-Control": "no-cache",
      },
    });
  } catch (err: any) {
    console.error("Error reading invoice:", err);
    return Response.json({ error: err.message }, { status: 500 });
  }
}
